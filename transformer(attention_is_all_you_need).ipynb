{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP77rCYFsbtfs0Efj9WSj8b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ac1ad5e92d641f8a7802e1976aacf07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97864ab583584f4aba083cce11ab8ae8",
              "IPY_MODEL_df966a48a8d642e182cb9ff0459cf4ac",
              "IPY_MODEL_d2a5cb773fc0493da5988aa3aff915f9"
            ],
            "layout": "IPY_MODEL_ed268a37912c4368ad9653a8619af09d"
          }
        },
        "97864ab583584f4aba083cce11ab8ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_250012763ff74e1f94da5fa26c915216",
            "placeholder": "​",
            "style": "IPY_MODEL_7ac9e0d25eab410d8dbd968a49667c52",
            "value": " 15%"
          }
        },
        "df966a48a8d642e182cb9ff0459cf4ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a7d840609143469e2589510b0f11c8",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03f94251edd24c1ebd4867431f138234",
            "value": 3
          }
        },
        "d2a5cb773fc0493da5988aa3aff915f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_245f30825eee484fa3fb14aaa20b6372",
            "placeholder": "​",
            "style": "IPY_MODEL_35b96cc0cc2e4a05b868157523b2f7c5",
            "value": " 3/20 [13:16&lt;1:14:49, 264.12s/it]"
          }
        },
        "ed268a37912c4368ad9653a8619af09d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "250012763ff74e1f94da5fa26c915216": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac9e0d25eab410d8dbd968a49667c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8a7d840609143469e2589510b0f11c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03f94251edd24c1ebd4867431f138234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "245f30825eee484fa3fb14aaa20b6372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35b96cc0cc2e4a05b868157523b2f7c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachit2005/Transformer/blob/main/transformer(attention_is_all_you_need).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dmodel = 512 --> in paper , it represents the size of the embedding vector of each word"
      ],
      "metadata": {
        "id": "vH8qL7QNSqL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input Embedding --> the process of converting input text (words or subwords) into numerical vectors, capturing their semantic meaning"
      ],
      "metadata": {
        "id": "jRNXIrznd0um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxkD9sLMX9_9",
        "outputId": "aa910734-78e2-4e4b-c55e-c1e6d5f2966c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q1JFssj91PSB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self, d_model:int, vocab_size:int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model) # given in paper under Embedding and Softmax title"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Embedding --> a technique used to inject information about the position of words in a sequence into the model's architecture"
      ],
      "metadata": {
        "id": "_mrcA5yGd3RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self , d_model,seq_length , dropout):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_length = seq_length\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    # create a matrix of shape (seq_length , d_model)\n",
        "    pe = torch.zeros(seq_length ,d_model)\n",
        "    # create a vector which represent the position of the word in the sentence\n",
        "    position = torch.arange(0,seq_length, dtype=torch.float).unsqueeze(1) # --> shape : [seq_length , 1]\n",
        "    div_term = torch.exp(torch.arange(0 , d_model , 2).float()*(-math.log(10000)/d_model)) # --> shape : [d_model/2]\n",
        "    # apply the sin to even pos and cos to odd pos\n",
        "    pe[:,0::2] = torch.sin(position * div_term) # --> all the columns with rows from 0 with step of 2\n",
        "    pe[:,1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    # now we add the batch dimension to apply to whole sentences\n",
        "    pe = pe.unsqueeze(0) # --> shape: [1,seq_length , d_model]\n",
        "    self.register_buffer('pe' , pe)\n",
        "\n",
        "  def forward(self , x):\n",
        "    # x.shape --> [batch_size, seq_length, d_model]\n",
        "    x = x + (self.pe[: , :x.shape[1] , :]).requires_grad_(False)\n",
        "    return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "DwlvzKYwdm9b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer Normalization --> normalization technique like batch normalization"
      ],
      "metadata": {
        "id": "AzMyUKM7kAve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self , epsilon:float = 10**-6):\n",
        "    super().__init__()\n",
        "    self.eps = epsilon\n",
        "    # nn.Parameter --> it is a special tensor that tells the model that it is a learnable parameter\n",
        "    self.gamma = nn.Parameter(torch.ones(1))\n",
        "    self.beta = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self , x):\n",
        "    mean = x.mean(dim=-1 , keepdim=True)\n",
        "    std = x.std(dim=-1 , keepdim=True)\n",
        "\n",
        "    gamma = self.gamma.to(x.device)\n",
        "    beta = self.beta.to(x.device)\n",
        "\n",
        "    return gamma*(x-mean)/(std + self.eps) + beta"
      ],
      "metadata": {
        "id": "8ngY7cxIj8RO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed Forward Layer --> This consists of two linear transformations(W1 , W2 , b1 , b2) with a ReLU activation(max-function) in between.\n",
        "\n",
        "FFN(x) = max(0xW1 +b1)W2 +b2\n",
        "\n",
        "and the first layer in from d_model to d_ff and then the other one is from d_ff to d_model"
      ],
      "metadata": {
        "id": "YYSUl-zhmDWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "  def __init__(self , d_model , d_ff , dropout):\n",
        "    super().__init__()\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(d_model , d_ff), # [batch , seq_length , d_model] --> [batch , seq_length , d_ff]\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(d_ff , d_model),# [batch , seq_length , d_ff] --> [batch , seq_length , d_model]\n",
        "    )\n",
        "\n",
        "  def forward(self , x):\n",
        "    return self.feed_forward(x)"
      ],
      "metadata": {
        "id": "V7nvrED0l5eQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Head-Attention --> a mechanism that enhances the original attention mechanism by running it multiple times in parallel, each with its own learnable parameters.\n",
        "\n",
        "please watch \"https://www.youtube.com/watch?v=bCz4OMemCcA\""
      ],
      "metadata": {
        "id": "E3W-CMxXnxYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self , d_model , h , dropout):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = h\n",
        "    self.d_k = d_model // h\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    assert d_model%h == 0\n",
        "\n",
        "    self.w_q = nn.Linear(d_model , d_model)\n",
        "    self.w_k = nn.Linear(d_model , d_model)\n",
        "    self.w_v = nn.Linear(d_model , d_model)\n",
        "\n",
        "    self.w_o = nn.Linear(h*self.d_k, d_model) # h*d_k == d_model\n",
        "\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query , key , value ,mask , dropout):\n",
        "    d_k = query.shape[-1]\n",
        "    # remember --> key shape: [batch , num_heads , seq_length , d_k] , after transpose --> [batch , num_heads , d_k ,seq_length]\n",
        "    attention_scores = (query @ key.transpose(-2 , -1))/d_k**(0.5) # --> [batch , num_heads , seq_length,seq_length]\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask==0 , -1e9)\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    return attention_scores @ value , attention_scores # --> shapes -> [batch , num_heads , seq_length , d_k] , [batch , num_heads , seq_length,seq_length]\n",
        "\n",
        "  def forward(self, q,k,v, mask):\n",
        "    # q.shape --> [batch , seq_length , d_model]\n",
        "    query = self.w_q(q) # --> [batch , seq_length , d_model]\n",
        "    key = self.w_k(k) # --> [batch , seq_length , d_model]\n",
        "    value = self.w_v(v) # --> [batch , seq_length , d_model]\n",
        "\n",
        "    # [batch , seq_length , d_model] --> [batch , seq_length , num_heads , d_k] --> [batch , num_heads , seq_length , d_k]\n",
        "    query = query.view(query.shape[0] , query.shape[1] , self.num_heads , self.d_k).transpose(1,2)\n",
        "    key = key.view(key.shape[0] , key.shape[1] , self.num_heads , self.d_k).transpose(1,2)\n",
        "    value = value.view(value.shape[0] , value.shape[1] , self.num_heads , self.d_k).transpose(1,2)\n",
        "\n",
        "    x , attention_scores = MultiheadAttention.attention(query , key , value , mask , self.dropout)\n",
        "\n",
        "    # [batch , num_heads , seq_length , d_k] --> [batch , seq_length , num_heads , d_k] --> [batch , seq_length , d_model]\n",
        "    x = x.transpose(1,2).contiguous().view(x.shape[0] , -1 , self.d_model)\n",
        "\n",
        "    return self.w_o(x) # shape --> [batch , seq_length , d_model]"
      ],
      "metadata": {
        "id": "ioTwprK1nwY6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self , dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x , sublayer):\n",
        "    # sublayer is the prev layer\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "UNsMh9VNwRHI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "DFenhHxx0cix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a Transformer architecture, the encoder's key and value tensors are used by the decoder"
      ],
      "metadata": {
        "id": "RDx3X7_638t7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self ,self_attention_block:MultiheadAttention , feed_forward_block:FeedForwardLayer, dropout):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    # first we do the self attention --> making the words intereact with each other in the same sentences\n",
        "    x = self.residual_connections[0](x , lambda x: self.self_attention_block(x,x,x,mask))\n",
        "    x = self.residual_connections[1](x , self.feed_forward_block)\n",
        "\n",
        "    return x # now this will go to the decoder as key and value pair"
      ],
      "metadata": {
        "id": "tS7Ezr3VxAdt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self , layers:nn.Module):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self , x ,mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x , mask)\n",
        "\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "vlY1GH4c3_cy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self , self_attention_block:MultiheadAttention , cross_attention_block:MultiheadAttention , feed_forward_block:FeedForwardLayer, dropout):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = [ResidualConnection(dropout) for _ in range(3)]\n",
        "\n",
        "  def forward(self , x , encoder_output , encoder_mask,decoder_mask):\n",
        "    x = self.residual_connections[0](x , lambda x: self.self_attention_block(x,x,x,decoder_mask))\n",
        "    x = self.residual_connections[1](x , lambda x:self.cross_attention_block(x,encoder_output,encoder_output,encoder_mask)) # in this query will come from the masked multi-head-attention and the key and value will come from the encoder block output\n",
        "    x = self.residual_connections[2](x , self.feed_forward_block)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "r7t5-oYl4oXT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self , layers:nn.Module):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self , x , encoder_output , encoder_mask , decoder_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x , encoder_output , encoder_mask , decoder_mask)\n",
        "\n",
        "    return self.norm(x)\n",
        "\n",
        "# we expect the output form decoder to be --> [batch , seq_length , d_model]"
      ],
      "metadata": {
        "id": "8cyfWedY630j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we want to map these words into the vocabulary\n",
        "class ProjectionLayer(nn.Module):\n",
        "  def __init__(self , d_model , vocab_size):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model , vocab_size)\n",
        "\n",
        "  def forward(self , x):\n",
        "    # x.shape --> [batch , seq_length , d_model]\n",
        "    return torch.log_softmax(self.proj(x) ,dim=-1) # --> [batch , seq_length , vocab_size]"
      ],
      "metadata": {
        "id": "Svwlv1Ej7v3_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Block  -->"
      ],
      "metadata": {
        "id": "c-t_qUY3-u5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self , encoder:Encoder , decoder:Decoder , src_emb:InputEmbedding , trg_emb:InputEmbedding , srcpos:PositionalEncoding , trgpos:PositionalEncoding , projection_layer:ProjectionLayer):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_emb = src_emb\n",
        "    self.trg_emb = trg_emb\n",
        "    self.srcpos = srcpos\n",
        "    self.trgpos = trgpos\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "  def encode(self , src , src_mask):\n",
        "    src = self.srcpos(self.src_emb(src))\n",
        "    return self.encoder(src , src_mask)\n",
        "\n",
        "  def decode(self , trg , encoder_output , src_mask , trg_mask):\n",
        "    trg = self.trgpos(self.trg_emb(trg))\n",
        "    return self.decoder(trg , encoder_output , src_mask , trg_mask)\n",
        "\n",
        "  def project(self , x):\n",
        "    return self.projection_layer(x)"
      ],
      "metadata": {
        "id": "49H0Z_jT-llD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size , trg_vocab_size , src_seq_length , trg_seq_length , d_model , h , dropout , N , d_ff):\n",
        "  # create embedding layer for source and target\n",
        "  src_embed = InputEmbedding(d_model , src_vocab_size + 1).to(device)\n",
        "  trg_embed = InputEmbedding(d_model , trg_vocab_size + 1).to(device)\n",
        "\n",
        "  # create positional embedding layer\n",
        "  src_pos = PositionalEncoding(d_model , src_seq_length , dropout).to(device)\n",
        "  trg_pos = PositionalEncoding(d_model , trg_seq_length , dropout).to(device)\n",
        "\n",
        "  # create encoder block\n",
        "  encoder_blocks = []\n",
        "  for _ in range(N):\n",
        "    encoder_self_attention_block = MultiheadAttention(d_model , h , dropout).to(device)\n",
        "    feed_forward_block = FeedForwardLayer(d_model , d_ff , dropout).to(device)\n",
        "    encoder_blocks.append(EncoderBlock(encoder_self_attention_block , feed_forward_block , dropout))\n",
        "\n",
        "  # create decoder block\n",
        "  decoder_blocks = []\n",
        "  for _ in range(N):\n",
        "    decoder_self_attention_block = MultiheadAttention(d_model , h , dropout).to(device)\n",
        "    decoder_cross_attention_block = MultiheadAttention(d_model , h , dropout).to(device)\n",
        "    feed_forward_block = FeedForwardLayer(d_model , d_ff , dropout).to(device)\n",
        "    decoder_blocks.append(DecoderBlock(decoder_self_attention_block , decoder_cross_attention_block , feed_forward_block , dropout))\n",
        "\n",
        "  # create the encoder and decoder\n",
        "  encoder = Encoder(encoder_blocks).to(device)\n",
        "  decoder = Decoder(decoder_blocks).to(device)\n",
        "\n",
        "  # create projection layer\n",
        "  projection_layer = ProjectionLayer(d_model , trg_vocab_size).to(device)\n",
        "\n",
        "  # create transformer\n",
        "  transformer = Transformer(encoder , decoder , src_embed , trg_embed , src_pos , trg_pos , projection_layer).to(device)\n",
        "\n",
        "  # initialize parameters using xavier_uniform_\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return transformer"
      ],
      "metadata": {
        "id": "7XD699IwVTVz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
        "!wget -nc https://downloads.tatoeba.org/exports/links.tar.bz2\n",
        "!tar -xf sentences.tar.bz2\n",
        "!tar -xf links.tar.bz2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEv8cj3VyGvF",
        "outputId": "f2ead0d1-3200-4487-82b0-5cfd05be5694"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘sentences.tar.bz2’ already there; not retrieving.\n",
            "\n",
            "File ‘links.tar.bz2’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"sentences.csv\", sep=\"\\t\", header=None, names=[\"id\", \"lang\", \"text\"])\n",
        "# print(df.head())\n",
        "\n",
        "df_eng = df[df['lang']=='eng']\n",
        "df_hindi = df[df['lang'] == 'hin']\n",
        "\n",
        "# print(df_eng.head())\n",
        "# print(df_hindi.head())\n",
        "\n",
        "links = pd.read_csv('links.csv' , sep='\\t' , header=None , names=[\"src\" , \"tgt\"]) # loads the links bettwen the target\n",
        "# print(links.head())\n",
        "\n",
        "# Merge to get aligned pairs\n",
        "merged = links.merge(df_eng, left_on=\"src\", right_on=\"id\").merge(df_hindi, left_on=\"tgt\", right_on=\"id\", suffixes=('_en', '_hi'))\n",
        "print(merged.head())\n",
        "\n",
        "en_sentences = merged['text_en'].tolist()\n",
        "hi_sentences = merged['text_hi'].tolist()\n",
        "\n",
        "print(f\"Found {len(en_sentences)} English–Hindi sentence pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP02uWRHyHYL",
        "outputId": "e598b019-98f3-4ad6-d040-e4259bc0c9a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    src      tgt  id_en lang_en                     text_en    id_hi lang_hi  \\\n",
            "0  1277  3792910   1277     eng      I have to go to sleep.  3792910     hin   \n",
            "1  1282   485968   1282     eng          Muiriel is 20 now.   485968     hin   \n",
            "2  1282  2060319   1282     eng          Muiriel is 20 now.  2060319     hin   \n",
            "3  1283   451291   1283     eng  The password is \"Muiriel\".   451291     hin   \n",
            "4  1283   451292   1283     eng  The password is \"Muiriel\".   451292     hin   \n",
            "\n",
            "                            text_hi  \n",
            "0                     मुझे सोना है।  \n",
            "1  म्यूरियल अब बीस साल की हो गई है।  \n",
            "2        म्यूरियल अब बीस साल की है।  \n",
            "3              कूटशब्द \"Muriel\" है।  \n",
            "4              पासवर्ड \"Muriel\" है।  \n",
            "Found 13182 English–Hindi sentence pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# converting the lines to txt file\n",
        "with open(\"eng.txt\" , \"w\") as f:\n",
        "  for line in en_sentences:\n",
        "    f.write(line.strip() + \"\\n\")\n",
        "\n",
        "with open(\"hindi.txt\" , \"w\") as f:\n",
        "  for line in hi_sentences:\n",
        "    f.write(line.strip() + \"\\n\")"
      ],
      "metadata": {
        "id": "FgyeDvj30WGp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(file_path:Path , vocab_size:int , output_path:Path):\n",
        "  tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  trainer = WordLevelTrainer(vocab_size=vocab_size , special_tokens=[\"[PAD]\" , \"[SOS]\" , \"[EOS]\" , \"[UNK]\"] , min_frequency=2)\n",
        "  tokenizer.train([file_path] , trainer)\n",
        "  tokenizer.save(output_path)\n",
        "\n",
        "train_tokenizer(\"eng.txt\" , 8000 , \"en-tokenize.json\")\n",
        "train_tokenizer(\"hindi.txt\" , 8000 , \"hindi-tokenize.json\")\n",
        "\n",
        "eng_tokenizer = Tokenizer.from_file(\"en-tokenize.json\")\n",
        "hindi_tokenizer = Tokenizer.from_file(\"hindi-tokenize.json\")\n",
        "\n",
        "VOCAB_SIZE_ENG = eng_tokenizer.get_vocab_size()     # 5912\n",
        "VOCAB_SIZE_HINDI = hindi_tokenizer.get_vocab_size() # 7070"
      ],
      "metadata": {
        "id": "CTdjXPig0D8M"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating a dataset"
      ],
      "metadata": {
        "id": "1ZA780iA4Sgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length_for_example = 5\n",
        "\n",
        "a = torch.triu(torch.ones((1,seq_length_for_example,seq_length_for_example)) , diagonal=1)\n",
        "print(a)\n",
        "print(a.shape)\n",
        "\n",
        "# to create a proper mask for decoder\n",
        "print((a == 0).int())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N86ewtBO8yNU",
        "outputId": "2b1f3ef4-0ebd-4912-f68b-0607a600868c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0., 1., 1., 1., 1.],\n",
            "         [0., 0., 1., 1., 1.],\n",
            "         [0., 0., 0., 1., 1.],\n",
            "         [0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0.]]])\n",
            "torch.Size([1, 5, 5])\n",
            "tensor([[[1, 0, 0, 0, 0],\n",
            "         [1, 1, 0, 0, 0],\n",
            "         [1, 1, 1, 0, 0],\n",
            "         [1, 1, 1, 1, 0],\n",
            "         [1, 1, 1, 1, 1]]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "ds = [\n",
        "    {\"translation\": {\"en\": en, \"hi\": hi}} for en, hi in zip(en_sentences, hi_sentences)\n",
        "]\n",
        "\n",
        "def casual_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0\n",
        "\n",
        "class Eng_Hindi_Dataset(Dataset):\n",
        "    def __init__(self, ds, device, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_length):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.ds = ds\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Get special token IDs and validate them\n",
        "        self.sos_token_src = self._get_token_id(tokenizer_src, \"[SOS]\")\n",
        "        self.eos_token_src = self._get_token_id(tokenizer_src, \"[EOS]\")\n",
        "        self.pad_token_src = self._get_token_id(tokenizer_src, \"[PAD]\")\n",
        "\n",
        "        self.sos_token_tgt = self._get_token_id(tokenizer_tgt, \"[SOS]\")\n",
        "        self.eos_token_tgt = self._get_token_id(tokenizer_tgt, \"[EOS]\")\n",
        "        self.pad_token_tgt = self._get_token_id(tokenizer_tgt, \"[PAD]\")\n",
        "\n",
        "        # Store vocab sizes for validation\n",
        "        self.src_vocab_size = tokenizer_src.get_vocab_size()\n",
        "        self.tgt_vocab_size = tokenizer_tgt.get_vocab_size()\n",
        "\n",
        "    def _get_token_id(self, tokenizer, token):\n",
        "        \"\"\"Helper method to get token ID with error handling\"\"\"\n",
        "        token_id = tokenizer.token_to_id(token)\n",
        "        if token_id is None:\n",
        "            raise ValueError(f\"Token '{token}' not found in tokenizer vocabulary\")\n",
        "        return token_id\n",
        "\n",
        "    def _validate_token_ids(self, token_ids, vocab_size, tokenizer_name):\n",
        "        \"\"\"Validate that all token IDs are within vocabulary range\"\"\"\n",
        "        if not token_ids:\n",
        "          return\n",
        "\n",
        "        min_id = min(token_ids)\n",
        "        max_id = max(token_ids)\n",
        "        if min_id < 0:\n",
        "             raise ValueError(f\"Negative token ID {min_id} found in {tokenizer_name} tokens.\")\n",
        "        if max_id >= vocab_size:\n",
        "            raise ValueError(f\"Token ID {max_id} exceeds {tokenizer_name} vocabulary size {vocab_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        src_target_pair = self.ds[index]\n",
        "\n",
        "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
        "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
        "\n",
        "        # Encode texts\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # Validate token IDs are within vocabulary range\n",
        "        self._validate_token_ids(enc_input_tokens, self.src_vocab_size, \"source\")\n",
        "        self._validate_token_ids(dec_input_tokens, self.tgt_vocab_size, \"target\")\n",
        "\n",
        "        # Calculate padding\n",
        "        enc_num_pad = self.seq_length - len(enc_input_tokens) - 2  # -2 for SOS and EOS\n",
        "        dec_num_pad = self.seq_length - len(dec_input_tokens) - 1  # -1 for EOS\n",
        "\n",
        "        if enc_num_pad < 0 or dec_num_pad < 0:\n",
        "            raise ValueError(f\"Sentence is too long. Source: {len(enc_input_tokens)}, Target: {len(dec_input_tokens)}, Max length: {self.seq_length}\")\n",
        "\n",
        "        # Create encoder input: [SOS] + tokens + [EOS] + padding\n",
        "        encoder_input = torch.cat([\n",
        "            torch.tensor([self.sos_token_src], dtype=torch.long),\n",
        "            torch.tensor(enc_input_tokens, dtype=torch.long),\n",
        "            torch.tensor([self.eos_token_src], dtype=torch.long),\n",
        "            torch.tensor([self.pad_token_src] * enc_num_pad, dtype=torch.long),\n",
        "        ])\n",
        "\n",
        "        # Create decoder input: [SOS] + tokens + padding\n",
        "        decoder_input = torch.cat([\n",
        "            torch.tensor([self.sos_token_tgt], dtype=torch.long),\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.long),\n",
        "            torch.tensor([self.pad_token_tgt] * dec_num_pad, dtype=torch.long),\n",
        "        ])\n",
        "\n",
        "        # Create labels: tokens + [EOS] + padding\n",
        "        label = torch.cat([\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.long),\n",
        "            torch.tensor([self.eos_token_tgt], dtype=torch.long),\n",
        "            torch.tensor([self.pad_token_tgt] * dec_num_pad, dtype=torch.long),\n",
        "        ])\n",
        "\n",
        "        # Verify all tensors have correct length\n",
        "        assert encoder_input.size(0) == self.seq_length, f\"Encoder input size: {encoder_input.size(0)}, expected: {self.seq_length}\"\n",
        "        assert decoder_input.size(0) == self.seq_length, f\"Decoder input size: {decoder_input.size(0)}, expected: {self.seq_length}\"\n",
        "        assert label.size(0) == self.seq_length, f\"Label size: {label.size(0)}, expected: {self.seq_length}\"\n",
        "\n",
        "        # Create masks\n",
        "        encoder_mask = (encoder_input != self.pad_token_src).unsqueeze(0).unsqueeze(0).int()\n",
        "\n",
        "        # Decoder mask: padding mask AND causal mask\n",
        "        decoder_padding_mask = (decoder_input != self.pad_token_tgt).unsqueeze(0).int()\n",
        "        decoder_causal_mask = casual_mask(decoder_input.size(0)).int()\n",
        "        decoder_mask = decoder_padding_mask & decoder_causal_mask\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input.to(device),\n",
        "            \"decoder_input\": decoder_input.to(device),\n",
        "            \"label\": label.to(device),\n",
        "            \"encoder_mask\": encoder_mask.to(self.device),\n",
        "            \"decoder_mask\": decoder_mask.to(self.device),\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n",
        "\n",
        "\n",
        "# Configuration\n",
        "BATCH_SIZE = 4\n",
        "SEQ_LENGTH = 350\n",
        "\n",
        "# Create dataset\n",
        "dataset = Eng_Hindi_Dataset(ds, device, eng_tokenizer, hindi_tokenizer, \"en\", \"hi\", SEQ_LENGTH)\n",
        "\n",
        "\n",
        "# Split dataset\n",
        "n = int(0.8 * len(dataset))\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [n, len(dataset) - n])\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Test the dataloader\n",
        "print(\"Testing dataloader...\")\n",
        "for i, batch in enumerate(train_dataloader):\n",
        "    if i >= 5: # Print for a few batches to see if the issue is consistent\n",
        "        break\n",
        "    print(f\"Processing batch {i}...\")\n",
        "    print(f\"Encoder input shape: {batch['encoder_input'].shape}\")\n",
        "    print(f\"Decoder input shape: {batch['decoder_input'].shape}\")\n",
        "    print(f\"Label shape: {batch['label'].shape}\")\n",
        "    print(f\"Encoder mask shape: {batch['encoder_mask'].shape}\")\n",
        "    print(f\"Decoder mask shape: {batch['decoder_mask'].shape}\")\n",
        "\n",
        "    # The added print statements in __getitem__ will also execute here for each item in the batch\n",
        "    break # Break after the first batch to see the output from the first item\n",
        "print(\"Dataloader test completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s20Vf5wVyjOr",
        "outputId": "6effe333-a61a-42bf-e2c4-b7374123fa9c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing dataloader...\n",
            "Processing batch 0...\n",
            "Encoder input shape: torch.Size([4, 350])\n",
            "Decoder input shape: torch.Size([4, 350])\n",
            "Label shape: torch.Size([4, 350])\n",
            "Encoder mask shape: torch.Size([4, 1, 1, 350])\n",
            "Decoder mask shape: torch.Size([4, 1, 350, 350])\n",
            "Dataloader test completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the transformer"
      ],
      "metadata": {
        "id": "gLBjZlOrBhNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade sympy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lYogaesuHQgH",
        "outputId": "0376d942-472f-4d88-f166-e547fc81f088"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.14.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE_ENG = 5912  # eng_tokenizer.get_vocab_size()\n",
        "VOCAB_SIZE_HINDI = 7070 # hindi_tokenizer.get_vocab_size()\n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 20\n",
        "LR = 10**-4\n",
        "D_model = 128\n",
        "D_ff = 2048//16\n",
        "H = 8 # no of heads\n",
        "N = 6 # no of layers\n",
        "DROPOUT = 0.1\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "losses = [0]\n",
        "\n",
        "model = build_transformer(VOCAB_SIZE_ENG , VOCAB_SIZE_HINDI  , SEQ_LENGTH , SEQ_LENGTH , D_model , H , DROPOUT , N , D_ff).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters() , LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=hindi_tokenizer.token_to_id(\"[PAD]\") , label_smoothing=0.1).to(device)\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "for epoch in tqdm(range(NUM_EPOCHS)):\n",
        "  print(f\"epoch : {epoch} || loss: {losses[-1]}\")\n",
        "  model.train()\n",
        "  for batch in train_dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    encoder_input = batch[\"encoder_input\"].long().to(device)  # Convert to Long type\n",
        "    decoder_input = batch[\"decoder_input\"].long().to(device)  # Convert to Long type\n",
        "    label = batch[\"label\"].long().to(device)  # Convert to Long type --> [32 , 350]\n",
        "    encoder_mask = batch[\"encoder_mask\"].to(device)\n",
        "    decoder_mask = batch[\"decoder_mask\"].to(device)\n",
        "\n",
        "    encoder_output = model.encode(encoder_input , encoder_mask)\n",
        "    decoder_output = model.decode(decoder_input , encoder_output , encoder_mask , decoder_mask)\n",
        "    proj_output = model.projection_layer(decoder_output) # --> [32 , 350 , 7070]\n",
        "\n",
        "    loss = criterion(proj_output.view(-1 , proj_output.size(-1)) , label.view(-1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss)\n",
        "\n",
        "print(losses[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "3ac1ad5e92d641f8a7802e1976aacf07",
            "97864ab583584f4aba083cce11ab8ae8",
            "df966a48a8d642e182cb9ff0459cf4ac",
            "d2a5cb773fc0493da5988aa3aff915f9",
            "ed268a37912c4368ad9653a8619af09d",
            "250012763ff74e1f94da5fa26c915216",
            "7ac9e0d25eab410d8dbd968a49667c52",
            "c8a7d840609143469e2589510b0f11c8",
            "03f94251edd24c1ebd4867431f138234",
            "245f30825eee484fa3fb14aaa20b6372",
            "35b96cc0cc2e4a05b868157523b2f7c5"
          ]
        },
        "id": "jrIKp02ABXhz",
        "outputId": "e147db66-c05e-4d87-83d9-3ec7f122bbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ac1ad5e92d641f8a7802e1976aacf07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0 || loss: 0\n",
            "epoch : 1 || loss: 4.930094242095947\n",
            "epoch : 2 || loss: 4.534841537475586\n",
            "epoch : 3 || loss: 4.914765357971191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model , src , src_tokenizer , tgt_tokenizer , seq_length , device):\n",
        "  model.eval()\n",
        "  src_ids = src_tokenizer.encode(src).ids\n",
        "  src_tokens = [src_tokenizer._get_token_id(\"[SOS]\")] + src_ids + [src_tokenizer._get_token_id(\"[EOS]\")]\n",
        "  src_tokens += [src_tokenizer._get_token_id(\"[PAD]\")] * (seq_length - len(src_tokens))\n",
        "  encodr_input = torch.tensor(src_tokens , dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "  encoder_mask = (encodr_input != src_tokenizer._get_token_id(\"[PAD]\")).unsqueeze(0).unsqueeze(0).int().to(device)\n",
        "\n",
        "  tgt_tokens = [tgt_tokenizer._get_token_id(\"[SOS]\")]\n",
        "  for _ in range(seq_length):\n",
        "    decoder_input = torch.tensor(tgt_tokens , dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    tgt_mask = (decoder_input != tgt_tokenizer._get_token_id(\"[PAD]\")).unsqueeze(0).int().to(device)\n",
        "    tgt_casual_mask = casual_mask(decoder_input.size(1)).int().to(device)\n",
        "\n",
        "    decoder_mask = tgt_mask & tgt_casual_mask\n",
        "\n",
        "    # forward pass\n",
        "    with torch.no_grad():\n",
        "      output = model(decoder_input , encoder_output , encoder_mask , decoder_mask)\n",
        "\n",
        "    next_token_logits = output[0 , -1 , :]\n",
        "\n",
        "    next_token_id = torch.multinomial(next_token_logits , num_samples=1).item()\n",
        "    tgt_tokens.append(next_token_id)\n",
        "\n",
        "    if next_token_id == tgt_tokenizer._get_token_id(\"[EOS]\"):\n",
        "      break\n",
        "\n",
        "  decoder_token = tgt_tokenizer.decode(tgt_tokens[1:])\n",
        "  return decoder_token\n"
      ],
      "metadata": {
        "id": "wiVzOJdgV5xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BmKFZdbtyGs1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}