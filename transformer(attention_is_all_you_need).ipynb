{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMh0vGi9h16yXDMPKTrcVTF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachit2005/Transformer/blob/main/transformer(attention_is_all_you_need).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dmodel = 512 --> in paper , it represents the size of the embedding vector of each word"
      ],
      "metadata": {
        "id": "vH8qL7QNSqL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input Embedding --> the process of converting input text (words or subwords) into numerical vectors, capturing their semantic meaning"
      ],
      "metadata": {
        "id": "jRNXIrznd0um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxkD9sLMX9_9",
        "outputId": "43fc09b0-6d71-421b-c582-7505706832ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1JFssj91PSB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self, d_model:int, vocab_size:int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model) # given in paper under Embedding and Softmax title"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Embedding --> a technique used to inject information about the position of words in a sequence into the model's architecture"
      ],
      "metadata": {
        "id": "_mrcA5yGd3RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self , d_model,seq_length , dropout):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_length = seq_length\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    # create a matrix of shape (seq_length , d_model)\n",
        "    pe = torch.zeros(seq_length ,d_model)\n",
        "    # create a vector which represent the position of the word in the sentence\n",
        "    position = torch.arange(0,seq_length, dtype=torch.float).unsqueeze(1) # --> shape : [seq_length , 1]\n",
        "    div_term = torch.exp(torch.arange(0 , d_model , 2).float()*(-math.log(10000)/d_model)) # --> shape : [d_model/2]\n",
        "    # apply the sin to even pos and cos to odd pos\n",
        "    pe[:,0::2] = torch.sin(position * div_term) # --> all the columns with rows from 0 with step of 2\n",
        "    pe[:,1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    # now we add the batch dimension to apply to whole sentences\n",
        "    pe = pe.unsqueeze(0) # --> shape: [1,seq_length , d_model]\n",
        "    self.register_buffer('pe' , pe)\n",
        "\n",
        "  def forward(self , x):\n",
        "    # x.shape --> [batch_size, seq_length, d_model]\n",
        "    x = x + (self.pe[: , :x.shape[1] , :]).requires_grad_(False)\n",
        "    return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "DwlvzKYwdm9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer Normalization --> normalization technique like batch normalization"
      ],
      "metadata": {
        "id": "AzMyUKM7kAve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self , epsilon:float = 10**-6):\n",
        "    super().__init__()\n",
        "    self.eps = epsilon\n",
        "    # nn.Parameter --> it is a special tensor that tells the model that it is a learnable parameter\n",
        "    self.gamma = nn.Parameter(torch.ones(1))\n",
        "    self.beta = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self , x):\n",
        "    mean = x.mean(dim=-1 , keepdim=True)\n",
        "    std = x.std(dim=-1 , keepdim=True)\n",
        "\n",
        "    gamma = self.gamma.to(x.device)\n",
        "    beta = self.beta.to(x.device)\n",
        "\n",
        "    return gamma*(x-mean)/(std + self.eps) + beta"
      ],
      "metadata": {
        "id": "8ngY7cxIj8RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed Forward Layer --> This consists of two linear transformations(W1 , W2 , b1 , b2) with a ReLU activation(max-function) in between.\n",
        "\n",
        "FFN(x) = max(0xW1 +b1)W2 +b2\n",
        "\n",
        "and the first layer in from d_model to d_ff and then the other one is from d_ff to d_model"
      ],
      "metadata": {
        "id": "YYSUl-zhmDWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "  def __init__(self , d_model , d_ff , dropout):\n",
        "    super().__init__()\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(d_model , d_ff), # [batch , seq_length , d_model] --> [batch , seq_length , d_ff]\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(d_ff , d_model),# [batch , seq_length , d_ff] --> [batch , seq_length , d_model]\n",
        "    )\n",
        "\n",
        "  def forward(self , x):\n",
        "    return self.feed_forward(x)"
      ],
      "metadata": {
        "id": "V7nvrED0l5eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Head-Attention --> a mechanism that enhances the original attention mechanism by running it multiple times in parallel, each with its own learnable parameters.\n",
        "\n",
        "please watch \"https://www.youtube.com/watch?v=bCz4OMemCcA\""
      ],
      "metadata": {
        "id": "E3W-CMxXnxYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self , d_model , h , dropout):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = h\n",
        "    self.d_k = d_model // h\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    assert d_model%h == 0\n",
        "\n",
        "    self.w_q = nn.Linear(d_model , d_model)\n",
        "    self.w_k = nn.Linear(d_model , d_model)\n",
        "    self.w_v = nn.Linear(d_model , d_model)\n",
        "\n",
        "    self.w_o = nn.Linear(h*self.d_k, d_model) # h*d_k == d_model\n",
        "\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query , key , value ,mask , dropout):\n",
        "    d_k = query.shape[-1]\n",
        "    # remember --> key shape: [batch , num_heads , seq_length , d_k] , after transpose --> [batch , num_heads , d_k ,seq_length]\n",
        "    attention_scores = (query @ key.transpose(-2 , -1))//d_k**(0.5) # --> [batch , num_heads , seq_length,seq_length]\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask==0 , -1e9)\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    return attention_scores @ value , attention_scores # --> shapes -> [batch , num_heads , seq_length , d_k] , [batch , num_heads , seq_length,seq_length]\n",
        "\n",
        "  def forward(self, q,k,v, mask):\n",
        "    # q.shape --> [batch , seq_length , d_model]\n",
        "    query = self.w_q(q) # --> [batch , seq_length , d_model]\n",
        "    key = self.w_k(k) # --> [batch , seq_length , d_model]\n",
        "    value = self.w_v(v) # --> [batch , seq_length , d_model]\n",
        "\n",
        "    # [batch , seq_length , d_model] --> [batch , seq_length , num_heads , d_k] --> [batch , num_heads , seq_length , d_k]\n",
        "    query = query.view(query.shape[0] , query.shape[1] , self.num_heads , self.d_k).transpose(1,2)\n",
        "    key = key.view(key.shape[0] , key.shape[1] , self.num_heads , self.d_k).transpose(1,2)\n",
        "    value = value.view(value.shape[0] , value.shape[1] , self.num_heads , self.d_k).transpose(1,2)\n",
        "\n",
        "    x , attention_scores = MultiheadAttention.attention(query , key , value , mask , self.dropout)\n",
        "\n",
        "    # [batch , num_heads , seq_length , d_k] --> [batch , seq_length , num_heads , d_k] --> [batch , seq_length , d_model]\n",
        "    x = x.transpose(1,2).contiguous().view(x.shape[0] , -1 , self.d_model)\n",
        "\n",
        "    return self.w_o(x) # shape --> [batch , seq_length , d_model]"
      ],
      "metadata": {
        "id": "ioTwprK1nwY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self , dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x , sublayer):\n",
        "    # sublayer is the prev layer\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "UNsMh9VNwRHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "DFenhHxx0cix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a Transformer architecture, the encoder's key and value tensors are used by the decoder"
      ],
      "metadata": {
        "id": "RDx3X7_638t7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self ,self_attention_block:MultiheadAttention , feed_forward_block:FeedForwardLayer, dropout):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    # first we do the self attention --> making the words intereact with each other in the same sentences\n",
        "    x = self.residual_connections[0](x , lambda x: self.self_attention_block(x,x,x,mask))\n",
        "    x = self.residual_connections[1](x , self.feed_forward_block)\n",
        "\n",
        "    return x # now this will go to the decoder as key and value pair"
      ],
      "metadata": {
        "id": "tS7Ezr3VxAdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self , layers:nn.Module):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self , x ,mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x , mask)\n",
        "\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "vlY1GH4c3_cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self , self_attention_block:MultiheadAttention , cross_attention_block:MultiheadAttention , feed_forward_block:FeedForwardLayer, dropout):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = [ResidualConnection(dropout) for _ in range(3)]\n",
        "\n",
        "  def forward(self , x , encoder_output , encoder_mask,decoder_mask):\n",
        "    x = self.residual_connections[0](x , lambda x: self.self_attention_block(x,x,x,decoder_mask))\n",
        "    x = self.residual_connections[1](x , lambda x:self.cross_attention_block(x,encoder_output,encoder_output,encoder_mask)) # in this query will come from the masked multi-head-attention and the key and value will come from the encoder block output\n",
        "    x = self.residual_connections[2](x , self.feed_forward_block)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "r7t5-oYl4oXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self , layers:nn.Module):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self , x , encoder_output , encoder_mask , decoder_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x , encoder_output , encoder_mask , decoder_mask)\n",
        "\n",
        "    return self.norm(x)\n",
        "\n",
        "# we expect the output form decoder to be --> [batch , seq_length , d_model]"
      ],
      "metadata": {
        "id": "8cyfWedY630j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we want to map these words into the vocabulary\n",
        "class ProjectionLayer(nn.Module):\n",
        "  def __init__(self , d_model , vocab_size):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model , vocab_size)\n",
        "\n",
        "  def forward(self , x):\n",
        "    # x.shape --> [batch , seq_length , d_model]\n",
        "    return torch.log_softmax(self.proj(x) ,dim=-1) # --> [batch , seq_length , vocab_size]"
      ],
      "metadata": {
        "id": "Svwlv1Ej7v3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Block  -->"
      ],
      "metadata": {
        "id": "c-t_qUY3-u5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self , encoder:Encoder , decoder:Decoder , src_emb:InputEmbedding , trg_emb:InputEmbedding , srcpos:PositionalEncoding , trgpos:PositionalEncoding , projection_layer:ProjectionLayer):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_emb = src_emb\n",
        "    self.trg_emb = trg_emb\n",
        "    self.srcpos = srcpos\n",
        "    self.trgpos = trgpos\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "  def encode(self , src , src_mask):\n",
        "    src = self.srcpos(self.src_emb(src))\n",
        "    return self.encoder(src , src_mask)\n",
        "\n",
        "  def decode(self , trg , encoder_output , src_mask , trg_mask):\n",
        "    trg = self.trgpos(self.trg_emb(trg))\n",
        "    return self.decoder(trg , encoder_output , src_mask , trg_mask)\n",
        "\n",
        "  def project(self , x):\n",
        "    return self.projection_layer(x)"
      ],
      "metadata": {
        "id": "49H0Z_jT-llD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size , trg_vocab_size , src_seq_length , trg_seq_length , d_model , h , dropout , N , d_ff):\n",
        "  # create embedding layer for source and target\n",
        "  src_embed = InputEmbedding(d_model , src_vocab_size + 1).to(device)\n",
        "  trg_embed = InputEmbedding(d_model , trg_vocab_size + 1).to(device)\n",
        "\n",
        "  # create positional embedding layer\n",
        "  src_pos = PositionalEncoding(d_model , src_seq_length , dropout).to(device)\n",
        "  trg_pos = PositionalEncoding(d_model , trg_seq_length , dropout).to(device)\n",
        "\n",
        "  # create encoder block\n",
        "  encoder_blocks = []\n",
        "  for _ in range(N):\n",
        "    encoder_self_attention_block = MultiheadAttention(d_model , h , dropout).to(device)\n",
        "    feed_forward_block = FeedForwardLayer(d_model , d_ff , dropout).to(device)\n",
        "    encoder_blocks.append(EncoderBlock(encoder_self_attention_block , feed_forward_block , dropout))\n",
        "\n",
        "  # create decoder block\n",
        "  decoder_blocks = []\n",
        "  for _ in range(N):\n",
        "    decoder_self_attention_block = MultiheadAttention(d_model , h , dropout).to(device)\n",
        "    decoder_cross_attention_block = MultiheadAttention(d_model , h , dropout).to(device)\n",
        "    feed_forward_block = FeedForwardLayer(d_model , d_ff , dropout).to(device)\n",
        "    decoder_blocks.append(DecoderBlock(decoder_self_attention_block , decoder_cross_attention_block , feed_forward_block , dropout))\n",
        "\n",
        "  # create the encoder and decoder\n",
        "  encoder = Encoder(encoder_blocks).to(device)\n",
        "  decoder = Decoder(decoder_blocks).to(device)\n",
        "\n",
        "  # create projection layer\n",
        "  projection_layer = ProjectionLayer(d_model , trg_vocab_size).to(device)\n",
        "\n",
        "  # create transformer\n",
        "  transformer = Transformer(encoder , decoder , src_embed , trg_embed , src_pos , trg_pos , projection_layer).to(device)\n",
        "\n",
        "  # initialize parameters using xavier_uniform_\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return transformer"
      ],
      "metadata": {
        "id": "7XD699IwVTVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
        "!wget -nc https://downloads.tatoeba.org/exports/links.tar.bz2\n",
        "!tar -xf sentences.tar.bz2\n",
        "!tar -xf links.tar.bz2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEv8cj3VyGvF",
        "outputId": "01188e1e-582e-437a-fd48-ad495388b039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-10 10:48:54--  https://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
            "Resolving downloads.tatoeba.org (downloads.tatoeba.org)... 94.130.77.194\n",
            "Connecting to downloads.tatoeba.org (downloads.tatoeba.org)|94.130.77.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 204659331 (195M) [application/octet-stream]\n",
            "Saving to: ‘sentences.tar.bz2’\n",
            "\n",
            "sentences.tar.bz2   100%[===================>] 195.18M  24.7MB/s    in 9.0s    \n",
            "\n",
            "2025-06-10 10:49:04 (21.8 MB/s) - ‘sentences.tar.bz2’ saved [204659331/204659331]\n",
            "\n",
            "--2025-06-10 10:49:04--  https://downloads.tatoeba.org/exports/links.tar.bz2\n",
            "Resolving downloads.tatoeba.org (downloads.tatoeba.org)... 94.130.77.194\n",
            "Connecting to downloads.tatoeba.org (downloads.tatoeba.org)|94.130.77.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 139072198 (133M) [application/octet-stream]\n",
            "Saving to: ‘links.tar.bz2’\n",
            "\n",
            "links.tar.bz2       100%[===================>] 132.63M  24.9MB/s    in 6.3s    \n",
            "\n",
            "2025-06-10 10:49:11 (20.9 MB/s) - ‘links.tar.bz2’ saved [139072198/139072198]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"sentences.csv\", sep=\"\\t\", header=None, names=[\"id\", \"lang\", \"text\"])\n",
        "# print(df.head())\n",
        "\n",
        "df_eng = df[df['lang']=='eng']\n",
        "df_hindi = df[df['lang'] == 'hin']\n",
        "\n",
        "# print(df_eng.head())\n",
        "# print(df_hindi.head())\n",
        "\n",
        "links = pd.read_csv('links.csv' , sep='\\t' , header=None , names=[\"src\" , \"tgt\"]) # loads the links bettwen the target\n",
        "# print(links.head())\n",
        "\n",
        "# Merge to get aligned pairs\n",
        "merged = links.merge(df_eng, left_on=\"src\", right_on=\"id\").merge(df_hindi, left_on=\"tgt\", right_on=\"id\", suffixes=('_en', '_hi'))\n",
        "print(merged.head())\n",
        "\n",
        "en_sentences = merged['text_en'].tolist()\n",
        "hi_sentences = merged['text_hi'].tolist()\n",
        "\n",
        "print(f\"Found {len(en_sentences)} English–Hindi sentence pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP02uWRHyHYL",
        "outputId": "bd21e5f8-a6ee-4f57-a8d2-fc542fe04b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    src      tgt  id_en lang_en                     text_en    id_hi lang_hi  \\\n",
            "0  1277  3792910   1277     eng      I have to go to sleep.  3792910     hin   \n",
            "1  1282   485968   1282     eng          Muiriel is 20 now.   485968     hin   \n",
            "2  1282  2060319   1282     eng          Muiriel is 20 now.  2060319     hin   \n",
            "3  1283   451291   1283     eng  The password is \"Muiriel\".   451291     hin   \n",
            "4  1283   451292   1283     eng  The password is \"Muiriel\".   451292     hin   \n",
            "\n",
            "                            text_hi  \n",
            "0                     मुझे सोना है।  \n",
            "1  म्यूरियल अब बीस साल की हो गई है।  \n",
            "2        म्यूरियल अब बीस साल की है।  \n",
            "3              कूटशब्द \"Muriel\" है।  \n",
            "4              पासवर्ड \"Muriel\" है।  \n",
            "Found 13182 English–Hindi sentence pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# converting the lines to txt file\n",
        "with open(\"eng.txt\" , \"w\") as f:\n",
        "  for line in en_sentences:\n",
        "    f.write(line.strip() + \"\\n\")\n",
        "\n",
        "with open(\"hindi.txt\" , \"w\") as f:\n",
        "  for line in hi_sentences:\n",
        "    f.write(line.strip() + \"\\n\")"
      ],
      "metadata": {
        "id": "FgyeDvj30WGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(file_path:Path , vocab_size:int , output_path:Path):\n",
        "  tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  trainer = WordLevelTrainer(vocab_size=vocab_size , special_tokens=[\"[PAD]\" , \"[SOS]\" , \"[EOS]\" , \"[UNK]\"] , min_frequency=2)\n",
        "  tokenizer.train([file_path] , trainer)\n",
        "  tokenizer.save(output_path)\n",
        "\n",
        "train_tokenizer(\"eng.txt\" , 8000 , \"en-tokenize.json\")\n",
        "train_tokenizer(\"hindi.txt\" , 8000 , \"hindi-tokenize.json\")\n",
        "\n",
        "eng_tokenizer = Tokenizer.from_file(\"en-tokenize.json\")\n",
        "hindi_tokenizer = Tokenizer.from_file(\"hindi-tokenize.json\")\n",
        "\n",
        "VOCAB_SIZE_ENG = eng_tokenizer.get_vocab_size()     # 5912\n",
        "VOCAB_SIZE_HINDI = hindi_tokenizer.get_vocab_size() # 7070"
      ],
      "metadata": {
        "id": "CTdjXPig0D8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating a dataset"
      ],
      "metadata": {
        "id": "1ZA780iA4Sgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length_for_example = 5\n",
        "\n",
        "a = torch.triu(torch.ones((1,seq_length_for_example,seq_length_for_example)) , diagonal=1)\n",
        "print(a)\n",
        "print(a.shape)\n",
        "\n",
        "# to create a proper mask for decoder\n",
        "print((a == 0).int())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N86ewtBO8yNU",
        "outputId": "6af0ccc9-69d3-4717-bf3c-f3b646889acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0., 1., 1., 1., 1.],\n",
            "         [0., 0., 1., 1., 1.],\n",
            "         [0., 0., 0., 1., 1.],\n",
            "         [0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0.]]])\n",
            "torch.Size([1, 5, 5])\n",
            "tensor([[[1, 0, 0, 0, 0],\n",
            "         [1, 1, 0, 0, 0],\n",
            "         [1, 1, 1, 0, 0],\n",
            "         [1, 1, 1, 1, 0],\n",
            "         [1, 1, 1, 1, 1]]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "ds = [\n",
        "    {\"translation\": {\"en\": en, \"hi\": hi}} for en, hi in zip(en_sentences, hi_sentences)\n",
        "]\n",
        "\n",
        "def casual_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0\n",
        "\n",
        "class Eng_Hindi_Dataset(Dataset):\n",
        "    def __init__(self, ds, device, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_length):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.ds = ds\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Get special token IDs and validate them\n",
        "        self.sos_token_src = self._get_token_id(tokenizer_src, \"[SOS]\")\n",
        "        self.eos_token_src = self._get_token_id(tokenizer_src, \"[EOS]\")\n",
        "        self.pad_token_src = self._get_token_id(tokenizer_src, \"[PAD]\")\n",
        "\n",
        "        self.sos_token_tgt = self._get_token_id(tokenizer_tgt, \"[SOS]\")\n",
        "        self.eos_token_tgt = self._get_token_id(tokenizer_tgt, \"[EOS]\")\n",
        "        self.pad_token_tgt = self._get_token_id(tokenizer_tgt, \"[PAD]\")\n",
        "\n",
        "        # Store vocab sizes for validation\n",
        "        self.src_vocab_size = tokenizer_src.get_vocab_size()\n",
        "        self.tgt_vocab_size = tokenizer_tgt.get_vocab_size()\n",
        "\n",
        "    def _get_token_id(self, tokenizer, token):\n",
        "        \"\"\"Helper method to get token ID with error handling\"\"\"\n",
        "        token_id = tokenizer.token_to_id(token)\n",
        "        if token_id is None:\n",
        "            raise ValueError(f\"Token '{token}' not found in tokenizer vocabulary\")\n",
        "        return token_id\n",
        "\n",
        "    def _validate_token_ids(self, token_ids, vocab_size, tokenizer_name):\n",
        "        \"\"\"Validate that all token IDs are within vocabulary range\"\"\"\n",
        "        if not token_ids:\n",
        "          return\n",
        "\n",
        "        min_id = min(token_ids)\n",
        "        max_id = max(token_ids)\n",
        "        if min_id < 0:\n",
        "             raise ValueError(f\"Negative token ID {min_id} found in {tokenizer_name} tokens.\")\n",
        "        if max_id >= vocab_size:\n",
        "            raise ValueError(f\"Token ID {max_id} exceeds {tokenizer_name} vocabulary size {vocab_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        src_target_pair = self.ds[index]\n",
        "\n",
        "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
        "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
        "\n",
        "        # Encode texts\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # Validate token IDs are within vocabulary range\n",
        "        self._validate_token_ids(enc_input_tokens, self.src_vocab_size, \"source\")\n",
        "        self._validate_token_ids(dec_input_tokens, self.tgt_vocab_size, \"target\")\n",
        "\n",
        "        # Calculate padding\n",
        "        enc_num_pad = self.seq_length - len(enc_input_tokens) - 2  # -2 for SOS and EOS\n",
        "        dec_num_pad = self.seq_length - len(dec_input_tokens) - 1  # -1 for EOS\n",
        "\n",
        "        if enc_num_pad < 0 or dec_num_pad < 0:\n",
        "            raise ValueError(f\"Sentence is too long. Source: {len(enc_input_tokens)}, Target: {len(dec_input_tokens)}, Max length: {self.seq_length}\")\n",
        "\n",
        "        # Create encoder input: [SOS] + tokens + [EOS] + padding\n",
        "        encoder_input = torch.cat([\n",
        "            torch.tensor([self.sos_token_src], dtype=torch.long),\n",
        "            torch.tensor(enc_input_tokens, dtype=torch.long),\n",
        "            torch.tensor([self.eos_token_src], dtype=torch.long),\n",
        "            torch.tensor([self.pad_token_src] * enc_num_pad, dtype=torch.long),\n",
        "        ])\n",
        "\n",
        "        # Create decoder input: [SOS] + tokens + padding\n",
        "        decoder_input = torch.cat([\n",
        "            torch.tensor([self.sos_token_tgt], dtype=torch.long),\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.long),\n",
        "            torch.tensor([self.pad_token_tgt] * dec_num_pad, dtype=torch.long),\n",
        "        ])\n",
        "\n",
        "        # Create labels: tokens + [EOS] + padding\n",
        "        label = torch.cat([\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.long),\n",
        "            torch.tensor([self.eos_token_tgt], dtype=torch.long),\n",
        "            torch.tensor([self.pad_token_tgt] * dec_num_pad, dtype=torch.long),\n",
        "        ])\n",
        "\n",
        "        # Verify all tensors have correct length\n",
        "        assert encoder_input.size(0) == self.seq_length, f\"Encoder input size: {encoder_input.size(0)}, expected: {self.seq_length}\"\n",
        "        assert decoder_input.size(0) == self.seq_length, f\"Decoder input size: {decoder_input.size(0)}, expected: {self.seq_length}\"\n",
        "        assert label.size(0) == self.seq_length, f\"Label size: {label.size(0)}, expected: {self.seq_length}\"\n",
        "\n",
        "        # Create masks\n",
        "        encoder_mask = (encoder_input != self.pad_token_src).unsqueeze(0).unsqueeze(0).int()\n",
        "\n",
        "        # Decoder mask: padding mask AND causal mask\n",
        "        decoder_padding_mask = (decoder_input != self.pad_token_tgt).unsqueeze(0).int()\n",
        "        decoder_causal_mask = casual_mask(decoder_input.size(0)).int()\n",
        "        decoder_mask = decoder_padding_mask & decoder_causal_mask\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input.to(device),\n",
        "            \"decoder_input\": decoder_input.to(device),\n",
        "            \"label\": label.to(device),\n",
        "            \"encoder_mask\": encoder_mask.to(self.device),\n",
        "            \"decoder_mask\": decoder_mask.to(self.device),\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n",
        "\n",
        "\n",
        "# Configuration\n",
        "BATCH_SIZE = 32\n",
        "SEQ_LENGTH = 350\n",
        "\n",
        "# Create dataset\n",
        "ds = ds[:5000]\n",
        "dataset = Eng_Hindi_Dataset(ds, device, eng_tokenizer, hindi_tokenizer, \"en\", \"hi\", SEQ_LENGTH)\n",
        "\n",
        "\n",
        "# Split dataset\n",
        "n = int(0.8 * len(dataset))\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [n, len(dataset) - n])\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Test the dataloader\n",
        "print(\"Testing dataloader...\")\n",
        "for i, batch in enumerate(train_dataloader):\n",
        "    if i >= 5: # Print for a few batches to see if the issue is consistent\n",
        "        break\n",
        "    print(f\"Processing batch {i}...\")\n",
        "    print(f\"Encoder input shape: {batch['encoder_input'].shape}\")\n",
        "    print(f\"Decoder input shape: {batch['decoder_input'].shape}\")\n",
        "    print(f\"Label shape: {batch['label'].shape}\")\n",
        "    print(f\"Encoder mask shape: {batch['encoder_mask'].shape}\")\n",
        "    print(f\"Decoder mask shape: {batch['decoder_mask'].shape}\")\n",
        "\n",
        "    # The added print statements in __getitem__ will also execute here for each item in the batch\n",
        "    break # Break after the first batch to see the output from the first item\n",
        "print(\"Dataloader test completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s20Vf5wVyjOr",
        "outputId": "c961a5df-d8e3-4136-ae43-8d5d3a3e96e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing dataloader...\n",
            "Processing batch 0...\n",
            "Encoder input shape: torch.Size([32, 350])\n",
            "Decoder input shape: torch.Size([32, 350])\n",
            "Label shape: torch.Size([32, 350])\n",
            "Encoder mask shape: torch.Size([32, 1, 1, 350])\n",
            "Decoder mask shape: torch.Size([32, 1, 350, 350])\n",
            "Dataloader test completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the transformer"
      ],
      "metadata": {
        "id": "gLBjZlOrBhNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade sympy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lYogaesuHQgH",
        "outputId": "b8e07bd1-6a63-42c8-b88c-94557c57fd61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting sympy\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE_ENG = 5912  # eng_tokenizer.get_vocab_size()\n",
        "VOCAB_SIZE_HINDI = 7070 # hindi_tokenizer.get_vocab_size()\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 20\n",
        "LR = 10**-4\n",
        "D_model = 512\n",
        "D_ff = 2048\n",
        "H = 8 # no of heads\n",
        "N = 6 # no of layers\n",
        "DROPOUT = 0.1\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "losses = []\n",
        "\n",
        "model = build_transformer(VOCAB_SIZE_ENG , VOCAB_SIZE_HINDI  , SEQ_LENGTH , SEQ_LENGTH , D_model , H , DROPOUT , N , D_ff).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters() , LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=hindi_tokenizer.token_to_id(\"[PAD]\") , label_smoothing=0.1).to(device)\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "for epoch in tqdm(range(NUM_EPOCHS)):\n",
        "  model.train()\n",
        "  for batch in train_dataloader:\n",
        "    print(\"in batch loop\")\n",
        "    optimizer.zero_grad()\n",
        "    encoder_input = batch[\"encoder_input\"].long().to(device)  # Convert to Long type\n",
        "    decoder_input = batch[\"decoder_input\"].long().to(device)  # Convert to Long type\n",
        "    label = batch[\"label\"].long().to(device)  # Convert to Long type --> [32 , 350]\n",
        "    encoder_mask = batch[\"encoder_mask\"].to(device)\n",
        "    decoder_mask = batch[\"decoder_mask\"].to(device)\n",
        "\n",
        "    encoder_output = model.encode(encoder_input , encoder_mask)\n",
        "    decoder_output = model.decode(decoder_input , encoder_output , encoder_mask , decoder_mask)\n",
        "    proj_output = model.projection_layer(decoder_output) # --> [32 , 350 , 7070]\n",
        "\n",
        "    print(proj_output.view(-1 , proj_output.size(-1)) , label.view(-1))\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "jrIKp02ABXhz",
        "outputId": "a1bc5a0b-4f72-4155-a929-42c63e2eea88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 12340 has 14.72 GiB memory in use. Of the allocated memory 14.53 GiB is allocated by PyTorch, and 76.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2624079fe0c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE_ENG\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE_HINDI\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mD_model\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mDROPOUT\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mD_ff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhindi_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[PAD]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-65b4560df913>\u001b[0m in \u001b[0;36mbuild_transformer\u001b[0;34m(src_vocab_size, trg_vocab_size, src_seq_length, trg_seq_length, d_model, h, dropout, N, d_ff)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdecoder_self_attention_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdecoder_cross_attention_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mfeed_forward_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForwardLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mdecoder_blocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDecoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_self_attention_block\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdecoder_cross_attention_block\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfeed_forward_block\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 12340 has 14.72 GiB memory in use. Of the allocated memory 14.53 GiB is allocated by PyTorch, and 76.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BmKFZdbtyGs1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}