{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachit2005/Transformer/blob/main/transformer(attention_is_all_you_need).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH8qL7QNSqL_"
      },
      "source": [
        "dmodel = 512 --> in paper , it represents the size of the embedding vector of each word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRNXIrznd0um"
      },
      "source": [
        "Input Embedding --> the process of converting input text (words or subwords) into numerical vectors, capturing their semantic meaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxkD9sLMX9_9",
        "outputId": "5d308cd4-9edc-4bf3-ee09-6b4c914154e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1JFssj91PSB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self, d_model:int, vocab_size:int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model) # given in paper under Embedding and Softmax title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mrcA5yGd3RL"
      },
      "source": [
        "Positional Embedding --> a technique used to inject information about the position of words in a sequence into the model's architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwlvzKYwdm9b"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self , d_model,seq_length , dropout):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_length = seq_length\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    # create a matrix of shape (seq_length , d_model)\n",
        "    pe = torch.zeros(seq_length ,d_model)\n",
        "    # create a vector which represent the position of the word in the sentence\n",
        "    position = torch.arange(0,seq_length, dtype=torch.float).unsqueeze(1) # --> shape : [seq_length , 1]\n",
        "    div_term = torch.exp(torch.arange(0 , d_model , 2).float()*(-math.log(10000)/d_model)) # --> shape : [d_model/2]\n",
        "    # apply the sin to even pos and cos to odd pos\n",
        "    pe[:,0::2] = torch.sin(position * div_term) # --> all the columns with rows from 0 with step of 2\n",
        "    pe[:,1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    # now we add the batch dimension to apply to whole sentences\n",
        "    pe = pe.unsqueeze(0) # --> shape: [1,seq_length , d_model]\n",
        "    self.register_buffer('pe' , pe)\n",
        "\n",
        "  def forward(self , x):\n",
        "    # x.shape --> [batch_size, seq_length, d_model]\n",
        "    x = x + (self.pe[: , :x.shape[1] , :]).requires_grad_(False)\n",
        "    return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzMyUKM7kAve"
      },
      "source": [
        "Layer Normalization --> normalization technique like batch normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ngY7cxIj8RO"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self , epsilon:float = 10**-6):\n",
        "    super().__init__()\n",
        "    self.eps = epsilon\n",
        "    # nn.Parameter --> it is a special tensor that tells the model that it is a learnable parameter\n",
        "    self.gamma = nn.Parameter(torch.ones(1))\n",
        "    self.beta = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self , x):\n",
        "    mean = x.mean(dim=-1 , keepdim=True)\n",
        "    std = x.std(dim=-1 , keepdim=True)\n",
        "\n",
        "    gamma = self.gamma.to(x.device)\n",
        "    beta = self.beta.to(x.device)\n",
        "\n",
        "    return gamma*(x-mean)/(std + self.eps) + beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYSUl-zhmDWe"
      },
      "source": [
        "Feed Forward Layer --> This consists of two linear transformations(W1 , W2 , b1 , b2) with a ReLU activation(max-function) in between.\n",
        "\n",
        "FFN(x) = max(0xW1 +b1)W2 +b2\n",
        "\n",
        "and the first layer in from d_model to d_ff and then the other one is from d_ff to d_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7nvrED0l5eQ"
      },
      "outputs": [],
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "  def __init__(self , d_model , d_ff , dropout):\n",
        "    super().__init__()\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(d_model , d_ff), # [batch , seq_length , d_model] --> [batch , seq_length , d_ff]\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(d_ff , d_model),# [batch , seq_length , d_ff] --> [batch , seq_length , d_model]\n",
        "    )\n",
        "\n",
        "  def forward(self , x):\n",
        "    return self.feed_forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3W-CMxXnxYc"
      },
      "source": [
        "Multi-Head-Attention --> a mechanism that enhances the original attention mechanism by running it multiple times in parallel, each with its own learnable parameters.\n",
        "\n",
        "please watch \"https://www.youtube.com/watch?v=bCz4OMemCcA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioTwprK1nwY6"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self , d_model , h , dropout):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = h\n",
        "    self.d_k = d_model // h\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    assert d_model%h == 0\n",
        "\n",
        "    self.w_q = nn.Linear(d_model , d_model)\n",
        "    self.w_k = nn.Linear(d_model , d_model)\n",
        "    self.w_v = nn.Linear(d_model , d_model)\n",
        "\n",
        "    self.w_o = nn.Linear(h*self.d_k, d_model) # h*d_k == d_model\n",
        "\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query , key , value ,mask , dropout):\n",
        "    d_k = query.shape[-1]\n",
        "    # remember --> key shape: [batch , num_heads , seq_length , d_k] , after transpose --> [batch , num_heads , d_k ,seq_length]\n",
        "    attention_scores = (query @ key.transpose(-2 , -1))/d_k**(0.5) # --> [batch , num_heads , seq_length,seq_length]\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask==0 , -1e9)\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    return attention_scores @ value , attention_scores # --> shapes -> [batch , num_heads , seq_length , d_k] , [batch , num_heads , seq_length,seq_length]\n",
        "\n",
        "  def forward(self, q,k,v, mask):\n",
        "    # q.shape --> [batch , seq_length , d_model]\n",
        "    query = self.w_q(q) # --> [batch , seq_length , d_model]\n",
        "    key = self.w_k(k) # --> [batch , seq_length , d_model]\n",
        "    value = self.w_v(v) # --> [batch , seq_length , d_model]\n",
        "\n",
        "    # [batch , seq_length , d_model] --> [batch , seq_length , num_heads , d_k] --> [batch , num_heads , seq_length , d_k]\n",
        "    query = query.view(query.shape[0] , query.shape[1] , self.num_heads , self.d_k).transpose(1,2)\n",
        "    key = key.view(key.shape[0] , key.shape[1] , self.num_heads , self.d_k).transpose(1,2)\n",
        "    value = value.view(value.shape[0] , value.shape[1] , self.num_heads , self.d_k).transpose(1,2)\n",
        "\n",
        "    x , attention_scores = MultiheadAttention.attention(query , key , value , mask , self.dropout)\n",
        "\n",
        "    # [batch , num_heads , seq_length , d_k] --> [batch , seq_length , num_heads , d_k] --> [batch , seq_length , d_model]\n",
        "    x = x.transpose(1,2).contiguous().view(x.shape[0] , -1 , self.d_model)\n",
        "\n",
        "    return self.w_o(x) # shape --> [batch , seq_length , d_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNsMh9VNwRHI"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self , dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x , sublayer):\n",
        "    # sublayer is the prev layer\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFenhHxx0cix"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDx3X7_638t7"
      },
      "source": [
        "In a Transformer architecture, the encoder's key and value tensors are used by the decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS7Ezr3VxAdt"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self ,self_attention_block:MultiheadAttention , feed_forward_block:FeedForwardLayer, dropout):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    # first we do the self attention --> making the words intereact with each other in the same sentences\n",
        "    x = self.residual_connections[0](x , lambda x: self.self_attention_block(x,x,x,mask))\n",
        "    x = self.residual_connections[1](x , self.feed_forward_block)\n",
        "\n",
        "    return x # now this will go to the decoder as key and value pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlY1GH4c3_cy"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self , layers:nn.Module):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self , x ,mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x , mask)\n",
        "\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7t5-oYl4oXT"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self , self_attention_block:MultiheadAttention , cross_attention_block:MultiheadAttention , feed_forward_block:FeedForwardLayer, dropout):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = [ResidualConnection(dropout) for _ in range(3)]\n",
        "\n",
        "  def forward(self , x , encoder_output , encoder_mask,decoder_mask):\n",
        "    x = self.residual_connections[0](x , lambda x: self.self_attention_block(x,x,x,decoder_mask))\n",
        "    x = self.residual_connections[1](x , lambda x:self.cross_attention_block(x,encoder_output,encoder_output,encoder_mask)) # in this query will come from the masked multi-head-attention and the key and value will come from the encoder block output\n",
        "    x = self.residual_connections[2](x , self.feed_forward_block)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cyfWedY630j"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self , layers:nn.Module):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self , x , encoder_output , encoder_mask , decoder_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x , encoder_output , encoder_mask , decoder_mask)\n",
        "\n",
        "    return self.norm(x)\n",
        "\n",
        "# we expect the output form decoder to be --> [batch , seq_length , d_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Svwlv1Ej7v3_"
      },
      "outputs": [],
      "source": [
        "# now we want to map these words into the vocabulary\n",
        "class ProjectionLayer(nn.Module):\n",
        "  def __init__(self , d_model , vocab_size):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model , vocab_size)\n",
        "\n",
        "  def forward(self , x):\n",
        "    # x.shape --> [batch , seq_length , d_model]\n",
        "    return torch.log_softmax(self.proj(x) ,dim=-1) # --> [batch , seq_length , vocab_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-t_qUY3-u5n"
      },
      "source": [
        "# Transformer Block  -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49H0Z_jT-llD"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self , encoder:Encoder , decoder:Decoder , src_emb:InputEmbedding , trg_emb:InputEmbedding , srcpos:PositionalEncoding , trgpos:PositionalEncoding , projection_layer:ProjectionLayer):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_emb = src_emb\n",
        "    self.trg_emb = trg_emb\n",
        "    self.srcpos = srcpos\n",
        "    self.trgpos = trgpos\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "  def encode(self , src , src_mask):\n",
        "    src = self.srcpos(self.src_emb(src))\n",
        "    return self.encoder(src , src_mask)\n",
        "\n",
        "  def decode(self , trg , encoder_output , src_mask , trg_mask):\n",
        "    trg = self.trgpos(self.trg_emb(trg))\n",
        "    return self.decoder(trg , encoder_output , src_mask , trg_mask)\n",
        "\n",
        "  def project(self , x):\n",
        "    return self.projection_layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XD699IwVTVz"
      },
      "outputs": [],
      "source": [
        "def build_transformer(src_vocab_size , trg_vocab_size , src_seq_length , trg_seq_length , d_model , h , dropout , N , d_ff):\n",
        "  # create embedding layer for source and target\n",
        "  src_embed = InputEmbedding(d_model , src_vocab_size + 1).to(device)\n",
        "  trg_embed = InputEmbedding(d_model , trg_vocab_size + 1).to(device)\n",
        "\n",
        "  # create positional embedding layer\n",
        "  src_pos = PositionalEncoding(d_model , src_seq_length , dropout).to(device)\n",
        "  trg_pos = PositionalEncoding(d_model , trg_seq_length , dropout).to(device)\n",
        "\n",
        "  # create encoder block\n",
        "  encoder_blocks = []\n",
        "  for _ in range(N):\n",
        "    encoder_self_attention_block = MultiheadAttention(d_model , h , dropout).to(device)\n",
        "    feed_forward_block = FeedForwardLayer(d_model , d_ff , dropout).to(device)\n",
        "    encoder_blocks.append(EncoderBlock(encoder_self_attention_block , feed_forward_block , dropout))\n",
        "\n",
        "  # create decoder block\n",
        "  decoder_blocks = []\n",
        "  for _ in range(N):\n",
        "    decoder_self_attention_block = MultiheadAttention(d_model , h , dropout).to(device)\n",
        "    decoder_cross_attention_block = MultiheadAttention(d_model , h , dropout).to(device)\n",
        "    feed_forward_block = FeedForwardLayer(d_model , d_ff , dropout).to(device)\n",
        "    decoder_blocks.append(DecoderBlock(decoder_self_attention_block , decoder_cross_attention_block , feed_forward_block , dropout))\n",
        "\n",
        "  # create the encoder and decoder\n",
        "  encoder = Encoder(encoder_blocks).to(device)\n",
        "  decoder = Decoder(decoder_blocks).to(device)\n",
        "\n",
        "  # create projection layer\n",
        "  projection_layer = ProjectionLayer(d_model , trg_vocab_size).to(device)\n",
        "\n",
        "  # create transformer\n",
        "  transformer = Transformer(encoder , decoder , src_embed , trg_embed , src_pos , trg_pos , projection_layer).to(device)\n",
        "\n",
        "  # initialize parameters using xavier_uniform_\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEv8cj3VyGvF",
        "outputId": "3426f4a6-35e8-4aca-8333-43d22667218b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘sentences.tar.bz2’ already there; not retrieving.\n",
            "\n",
            "File ‘links.tar.bz2’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
        "!wget -nc https://downloads.tatoeba.org/exports/links.tar.bz2\n",
        "!tar -xf sentences.tar.bz2\n",
        "!tar -xf links.tar.bz2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP02uWRHyHYL",
        "outputId": "d9f8770c-c0fc-43a2-b7b9-1209d0bac61f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    src      tgt  id_en lang_en                     text_en    id_hi lang_hi  \\\n",
            "0  1277  3792910   1277     eng      I have to go to sleep.  3792910     hin   \n",
            "1  1282   485968   1282     eng          Muiriel is 20 now.   485968     hin   \n",
            "2  1282  2060319   1282     eng          Muiriel is 20 now.  2060319     hin   \n",
            "3  1283   451291   1283     eng  The password is \"Muiriel\".   451291     hin   \n",
            "4  1283   451292   1283     eng  The password is \"Muiriel\".   451292     hin   \n",
            "\n",
            "                            text_hi  \n",
            "0                     मुझे सोना है।  \n",
            "1  म्यूरियल अब बीस साल की हो गई है।  \n",
            "2        म्यूरियल अब बीस साल की है।  \n",
            "3              कूटशब्द \"Muriel\" है।  \n",
            "4              पासवर्ड \"Muriel\" है।  \n",
            "Found 13182 English–Hindi sentence pairs\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"sentences.csv\", sep=\"\\t\", header=None, names=[\"id\", \"lang\", \"text\"])\n",
        "# print(df.head())\n",
        "\n",
        "df_eng = df[df['lang']=='eng']\n",
        "df_hindi = df[df['lang'] == 'hin']\n",
        "\n",
        "# print(df_eng.head())\n",
        "# print(df_hindi.head())\n",
        "\n",
        "links = pd.read_csv('links.csv' , sep='\\t' , header=None , names=[\"src\" , \"tgt\"]) # loads the links bettwen the target\n",
        "# print(links.head())\n",
        "\n",
        "# Merge to get aligned pairs\n",
        "merged = links.merge(df_eng, left_on=\"src\", right_on=\"id\").merge(df_hindi, left_on=\"tgt\", right_on=\"id\", suffixes=('_en', '_hi'))\n",
        "print(merged.head())\n",
        "\n",
        "en_sentences = merged['text_en'].tolist()\n",
        "hi_sentences = merged['text_hi'].tolist()\n",
        "\n",
        "print(f\"Found {len(en_sentences)} English–Hindi sentence pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgyeDvj30WGp"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# converting the lines to txt file\n",
        "with open(\"eng.txt\" , \"w\") as f:\n",
        "  for line in en_sentences:\n",
        "    f.write(line.strip() + \"\\n\")\n",
        "\n",
        "with open(\"hindi.txt\" , \"w\") as f:\n",
        "  for line in hi_sentences:\n",
        "    f.write(line.strip() + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTdjXPig0D8M"
      },
      "outputs": [],
      "source": [
        "def train_tokenizer(file_path:Path , vocab_size:int , output_path:Path):\n",
        "  tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  trainer = WordLevelTrainer(vocab_size=vocab_size , special_tokens=[\"[PAD]\" , \"[SOS]\" , \"[EOS]\" , \"[UNK]\"] , min_frequency=2)\n",
        "  tokenizer.train([file_path] , trainer)\n",
        "  tokenizer.save(output_path)\n",
        "\n",
        "train_tokenizer(\"eng.txt\" , 8000 , \"en-tokenize.json\")\n",
        "train_tokenizer(\"hindi.txt\" , 8000 , \"hindi-tokenize.json\")\n",
        "\n",
        "eng_tokenizer = Tokenizer.from_file(\"en-tokenize.json\")\n",
        "hindi_tokenizer = Tokenizer.from_file(\"hindi-tokenize.json\")\n",
        "\n",
        "VOCAB_SIZE_ENG = eng_tokenizer.get_vocab_size()     # 5912\n",
        "VOCAB_SIZE_HINDI = hindi_tokenizer.get_vocab_size() # 7070"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZA780iA4Sgz"
      },
      "source": [
        "creating a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N86ewtBO8yNU",
        "outputId": "776f3ae4-c025-4123-c222-4575d9c9789f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0., 1., 1., 1., 1.],\n",
            "         [0., 0., 1., 1., 1.],\n",
            "         [0., 0., 0., 1., 1.],\n",
            "         [0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0.]]])\n",
            "torch.Size([1, 5, 5])\n",
            "tensor([[[1, 0, 0, 0, 0],\n",
            "         [1, 1, 0, 0, 0],\n",
            "         [1, 1, 1, 0, 0],\n",
            "         [1, 1, 1, 1, 0],\n",
            "         [1, 1, 1, 1, 1]]], dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "seq_length_for_example = 5\n",
        "\n",
        "a = torch.triu(torch.ones((1,seq_length_for_example,seq_length_for_example)) , diagonal=1)\n",
        "print(a)\n",
        "print(a.shape)\n",
        "\n",
        "# to create a proper mask for decoder\n",
        "print((a == 0).int())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s20Vf5wVyjOr",
        "outputId": "49c8c6ef-b719-4642-9364-4e5147b220a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing dataloader...\n",
            "Processing batch 0...\n",
            "Encoder input shape: torch.Size([32, 100])\n",
            "Decoder input shape: torch.Size([32, 100])\n",
            "Label shape: torch.Size([32, 100])\n",
            "Encoder mask shape: torch.Size([32, 1, 1, 100])\n",
            "Decoder mask shape: torch.Size([32, 1, 100, 100])\n",
            "Dataloader test completed successfully!\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "ds = [\n",
        "    {\"translation\": {\"en\": en, \"hi\": hi}} for en, hi in zip(en_sentences, hi_sentences)\n",
        "]\n",
        "\n",
        "def casual_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0\n",
        "\n",
        "class Eng_Hindi_Dataset(Dataset):\n",
        "    def __init__(self, ds, device, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_length):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.ds = ds\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Get special token IDs and validate them\n",
        "        self.sos_token_src = self._get_token_id(tokenizer_src, \"[SOS]\")\n",
        "        self.eos_token_src = self._get_token_id(tokenizer_src, \"[EOS]\")\n",
        "        self.pad_token_src = self._get_token_id(tokenizer_src, \"[PAD]\")\n",
        "\n",
        "        self.sos_token_tgt = self._get_token_id(tokenizer_tgt, \"[SOS]\")\n",
        "        self.eos_token_tgt = self._get_token_id(tokenizer_tgt, \"[EOS]\")\n",
        "        self.pad_token_tgt = self._get_token_id(tokenizer_tgt, \"[PAD]\")\n",
        "\n",
        "        # Store vocab sizes for validation\n",
        "        self.src_vocab_size = tokenizer_src.get_vocab_size()\n",
        "        self.tgt_vocab_size = tokenizer_tgt.get_vocab_size()\n",
        "\n",
        "    def _get_token_id(self, tokenizer, token):\n",
        "        \"\"\"Helper method to get token ID with error handling\"\"\"\n",
        "        token_id = tokenizer.token_to_id(token)\n",
        "        if token_id is None:\n",
        "            raise ValueError(f\"Token '{token}' not found in tokenizer vocabulary\")\n",
        "        return token_id\n",
        "\n",
        "    def _validate_token_ids(self, token_ids, vocab_size, tokenizer_name):\n",
        "        \"\"\"Validate that all token IDs are within vocabulary range\"\"\"\n",
        "        if not token_ids:\n",
        "          return\n",
        "\n",
        "        min_id = min(token_ids)\n",
        "        max_id = max(token_ids)\n",
        "        if min_id < 0:\n",
        "             raise ValueError(f\"Negative token ID {min_id} found in {tokenizer_name} tokens.\")\n",
        "        if max_id >= vocab_size:\n",
        "            raise ValueError(f\"Token ID {max_id} exceeds {tokenizer_name} vocabulary size {vocab_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        src_target_pair = self.ds[index]\n",
        "\n",
        "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
        "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
        "\n",
        "        # Encode texts\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # Validate token IDs are within vocabulary range\n",
        "        self._validate_token_ids(enc_input_tokens, self.src_vocab_size, \"source\")\n",
        "        self._validate_token_ids(dec_input_tokens, self.tgt_vocab_size, \"target\")\n",
        "\n",
        "        # Calculate padding\n",
        "        enc_num_pad = self.seq_length - len(enc_input_tokens) - 2  # -2 for SOS and EOS\n",
        "        dec_num_pad = self.seq_length - len(dec_input_tokens) - 1  # -1 for EOS\n",
        "\n",
        "        if enc_num_pad < 0 or dec_num_pad < 0:\n",
        "            raise ValueError(f\"Sentence is too long. Source: {len(enc_input_tokens)}, Target: {len(dec_input_tokens)}, Max length: {self.seq_length}\")\n",
        "\n",
        "        # Create encoder input: [SOS] + tokens + [EOS] + padding\n",
        "        encoder_input = torch.cat([\n",
        "            torch.tensor([self.sos_token_src], dtype=torch.long),\n",
        "            torch.tensor(enc_input_tokens, dtype=torch.long),\n",
        "            torch.tensor([self.eos_token_src], dtype=torch.long),\n",
        "            torch.tensor([self.pad_token_src] * enc_num_pad, dtype=torch.long),\n",
        "        ])\n",
        "\n",
        "        # Create decoder input: [SOS] + tokens + padding\n",
        "        decoder_input = torch.cat([\n",
        "            torch.tensor([self.sos_token_tgt], dtype=torch.long),\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.long),\n",
        "            torch.tensor([self.pad_token_tgt] * dec_num_pad, dtype=torch.long),\n",
        "        ])\n",
        "\n",
        "        # Create labels: tokens + [EOS] + padding\n",
        "        label = torch.cat([\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.long),\n",
        "            torch.tensor([self.eos_token_tgt], dtype=torch.long),\n",
        "            torch.tensor([self.pad_token_tgt] * dec_num_pad, dtype=torch.long),\n",
        "        ])\n",
        "\n",
        "        # Verify all tensors have correct length\n",
        "        assert encoder_input.size(0) == self.seq_length, f\"Encoder input size: {encoder_input.size(0)}, expected: {self.seq_length}\"\n",
        "        assert decoder_input.size(0) == self.seq_length, f\"Decoder input size: {decoder_input.size(0)}, expected: {self.seq_length}\"\n",
        "        assert label.size(0) == self.seq_length, f\"Label size: {label.size(0)}, expected: {self.seq_length}\"\n",
        "\n",
        "        # Create masks\n",
        "        encoder_mask = (encoder_input != self.pad_token_src).unsqueeze(0).unsqueeze(0).int()\n",
        "\n",
        "        # Decoder mask: padding mask AND causal mask\n",
        "        decoder_padding_mask = (decoder_input != self.pad_token_tgt).unsqueeze(0).int()\n",
        "        decoder_causal_mask = casual_mask(decoder_input.size(0)).int()\n",
        "        decoder_mask = decoder_padding_mask & decoder_causal_mask\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input.to(device),\n",
        "            \"decoder_input\": decoder_input.to(device),\n",
        "            \"label\": label.to(device),\n",
        "            \"encoder_mask\": encoder_mask.to(self.device),\n",
        "            \"decoder_mask\": decoder_mask.to(self.device),\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n",
        "\n",
        "\n",
        "# Configuration\n",
        "BATCH_SIZE = 32\n",
        "SEQ_LENGTH = 100\n",
        "\n",
        "# Create dataset\n",
        "dataset = Eng_Hindi_Dataset(ds, device, eng_tokenizer, hindi_tokenizer, \"en\", \"hi\", SEQ_LENGTH)\n",
        "\n",
        "\n",
        "# Split dataset\n",
        "n = int(0.8 * len(dataset))\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [n, len(dataset) - n])\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True , pin_memory=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Test the dataloader\n",
        "print(\"Testing dataloader...\")\n",
        "for i, batch in enumerate(train_dataloader):\n",
        "    if i >= 5: # Print for a few batches to see if the issue is consistent\n",
        "        break\n",
        "    print(f\"Processing batch {i}...\")\n",
        "    print(f\"Encoder input shape: {batch['encoder_input'].shape}\")\n",
        "    print(f\"Decoder input shape: {batch['decoder_input'].shape}\")\n",
        "    print(f\"Label shape: {batch['label'].shape}\")\n",
        "    print(f\"Encoder mask shape: {batch['encoder_mask'].shape}\")\n",
        "    print(f\"Decoder mask shape: {batch['decoder_mask'].shape}\")\n",
        "\n",
        "    # The added print statements in __getitem__ will also execute here for each item in the batch\n",
        "    break # Break after the first batch to see the output from the first item\n",
        "print(\"Dataloader test completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLBjZlOrBhNd"
      },
      "source": [
        "Training the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lYogaesuHQgH",
        "outputId": "9cb49060-38ed-44bf-b934-aa9b463e91c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.14.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade sympy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE_ENG = 5912  # eng_tokenizer.get_vocab_size()\n",
        "VOCAB_SIZE_HINDI = 7070 # hindi_tokenizer.get_vocab_size()\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "LR = 10**-4\n",
        "D_model = 128//2\n",
        "D_ff = 512//2\n",
        "H = 4 # no of heads\n",
        "N = 3 # no of layers\n",
        "DROPOUT = 0.1\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "losses = [0]\n",
        "\n",
        "model = build_transformer(VOCAB_SIZE_ENG , VOCAB_SIZE_HINDI  , SEQ_LENGTH , SEQ_LENGTH , D_model , H , DROPOUT , N , D_ff).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters() , LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=hindi_tokenizer.token_to_id(\"[PAD]\") , label_smoothing=0.1).to(device)"
      ],
      "metadata": {
        "id": "zDvXhILWxSxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "# Configs\n",
        "NUM_EPOCHS = 50\n",
        "ACCUM_STEPS = 4\n",
        "CHECKPOINT_PATH = 'checkpoint.pt'\n",
        "BEST_MODEL_PATH = 'best_model.pt'\n",
        "\n",
        "# Initialize\n",
        "start_epoch = 0\n",
        "losses = []\n",
        "best_loss = float('inf')  # Track best loss\n",
        "\n",
        "# Resume checkpoint if exists\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    print(\"🔁 Resuming from checkpoint...\")\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    losses = checkpoint['losses']\n",
        "    best_loss = checkpoint.get('best_loss', float('inf'))\n",
        "\n",
        "# Training loop\n",
        "for epoch in tqdm(range(NUM_EPOCHS), desc=\"Training\"):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        encoder_input = batch[\"encoder_input\"].long().to(device)\n",
        "        decoder_input = batch[\"decoder_input\"].long().to(device)\n",
        "        label = batch[\"label\"].long().to(device)\n",
        "        encoder_mask = batch[\"encoder_mask\"].to(device)\n",
        "        decoder_mask = batch[\"decoder_mask\"].to(device)\n",
        "\n",
        "        encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "        decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)\n",
        "        proj_output = model.projection_layer(decoder_output)\n",
        "\n",
        "        loss = criterion(proj_output.reshape(-1, proj_output.size(-1)), label.reshape(-1))\n",
        "        loss = loss / ACCUM_STEPS\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % ACCUM_STEPS == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataloader)\n",
        "    losses.append(epoch_loss)\n",
        "    print(f\"Epoch {epoch} || Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Save regular checkpoint\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'losses': losses,\n",
        "        'best_loss': best_loss\n",
        "    }, CHECKPOINT_PATH)\n",
        "\n",
        "    # Save only best model\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        print(f\"🏆 Best model saved at epoch {epoch} with loss {best_loss:.4f}\")\n",
        "\n",
        "print(f\"✅ Final Loss: {losses[-1]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9d2d5b5b8df349509b71d2e14faf32a1",
            "41aec81c130f44e392311317a933e260",
            "5f5b64997dad4dd18f0d060177fefa9c",
            "7c2390d44c58438691bf1d4a262b93e9",
            "afa99b0f44db4b94b190ba02abee68c2",
            "422fe1b0eb5f4645b90422eff6372791",
            "6bf64bb71b154b30a69079ce9fe9f47c",
            "97ba9fc5a75448d0bd7fb3434b51b300",
            "586db8fbd0db48929b6afa921fa5e8f4",
            "cb41b4cf0a044f798e23a63c3343cc4f",
            "08deb796708044978f01e651480174ec"
          ]
        },
        "id": "rrCqoCU8G-dr",
        "outputId": "a40ddecc-d088-448b-8f20-008137b9e393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔁 Resuming from checkpoint...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d2d5b5b8df349509b71d2e14faf32a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 || Loss: 0.8204\n",
            "🏆 Best model saved at epoch 0 with loss 0.8204\n",
            "Epoch 1 || Loss: 0.8185\n",
            "🏆 Best model saved at epoch 1 with loss 0.8185\n",
            "Epoch 2 || Loss: 0.8166\n",
            "🏆 Best model saved at epoch 2 with loss 0.8166\n",
            "Epoch 3 || Loss: 0.8151\n",
            "🏆 Best model saved at epoch 3 with loss 0.8151\n",
            "Epoch 4 || Loss: 0.8133\n",
            "🏆 Best model saved at epoch 4 with loss 0.8133\n",
            "Epoch 5 || Loss: 0.8113\n",
            "🏆 Best model saved at epoch 5 with loss 0.8113\n",
            "Epoch 6 || Loss: 0.8082\n",
            "🏆 Best model saved at epoch 6 with loss 0.8082\n",
            "Epoch 7 || Loss: 0.8068\n",
            "🏆 Best model saved at epoch 7 with loss 0.8068\n",
            "Epoch 8 || Loss: 0.8053\n",
            "🏆 Best model saved at epoch 8 with loss 0.8053\n",
            "Epoch 9 || Loss: 0.8038\n",
            "🏆 Best model saved at epoch 9 with loss 0.8038\n",
            "Epoch 10 || Loss: 0.8021\n",
            "🏆 Best model saved at epoch 10 with loss 0.8021\n",
            "Epoch 11 || Loss: 0.8005\n",
            "🏆 Best model saved at epoch 11 with loss 0.8005\n",
            "Epoch 12 || Loss: 0.7982\n",
            "🏆 Best model saved at epoch 12 with loss 0.7982\n",
            "Epoch 13 || Loss: 0.7967\n",
            "🏆 Best model saved at epoch 13 with loss 0.7967\n",
            "Epoch 14 || Loss: 0.7944\n",
            "🏆 Best model saved at epoch 14 with loss 0.7944\n",
            "Epoch 15 || Loss: 0.7934\n",
            "🏆 Best model saved at epoch 15 with loss 0.7934\n",
            "Epoch 16 || Loss: 0.7905\n",
            "🏆 Best model saved at epoch 16 with loss 0.7905\n",
            "Epoch 17 || Loss: 0.7895\n",
            "🏆 Best model saved at epoch 17 with loss 0.7895\n",
            "Epoch 18 || Loss: 0.7880\n",
            "🏆 Best model saved at epoch 18 with loss 0.7880\n",
            "Epoch 19 || Loss: 0.7870\n",
            "🏆 Best model saved at epoch 19 with loss 0.7870\n",
            "Epoch 20 || Loss: 0.7838\n",
            "🏆 Best model saved at epoch 20 with loss 0.7838\n",
            "Epoch 21 || Loss: 0.7829\n",
            "🏆 Best model saved at epoch 21 with loss 0.7829\n",
            "Epoch 22 || Loss: 0.7802\n",
            "🏆 Best model saved at epoch 22 with loss 0.7802\n",
            "Epoch 23 || Loss: 0.7781\n",
            "🏆 Best model saved at epoch 23 with loss 0.7781\n",
            "Epoch 24 || Loss: 0.7781\n",
            "Epoch 25 || Loss: 0.7769\n",
            "🏆 Best model saved at epoch 25 with loss 0.7769\n",
            "Epoch 26 || Loss: 0.7743\n",
            "🏆 Best model saved at epoch 26 with loss 0.7743\n",
            "Epoch 27 || Loss: 0.7729\n",
            "🏆 Best model saved at epoch 27 with loss 0.7729\n",
            "Epoch 28 || Loss: 0.7712\n",
            "🏆 Best model saved at epoch 28 with loss 0.7712\n",
            "Epoch 29 || Loss: 0.7703\n",
            "🏆 Best model saved at epoch 29 with loss 0.7703\n",
            "Epoch 30 || Loss: 0.7680\n",
            "🏆 Best model saved at epoch 30 with loss 0.7680\n",
            "Epoch 31 || Loss: 0.7670\n",
            "🏆 Best model saved at epoch 31 with loss 0.7670\n",
            "Epoch 32 || Loss: 0.7655\n",
            "🏆 Best model saved at epoch 32 with loss 0.7655\n",
            "Epoch 33 || Loss: 0.7635\n",
            "🏆 Best model saved at epoch 33 with loss 0.7635\n",
            "Epoch 34 || Loss: 0.7631\n",
            "🏆 Best model saved at epoch 34 with loss 0.7631\n",
            "Epoch 35 || Loss: 0.7617\n",
            "🏆 Best model saved at epoch 35 with loss 0.7617\n",
            "Epoch 36 || Loss: 0.7590\n",
            "🏆 Best model saved at epoch 36 with loss 0.7590\n",
            "Epoch 37 || Loss: 0.7582\n",
            "🏆 Best model saved at epoch 37 with loss 0.7582\n",
            "Epoch 38 || Loss: 0.7576\n",
            "🏆 Best model saved at epoch 38 with loss 0.7576\n",
            "Epoch 39 || Loss: 0.7562\n",
            "🏆 Best model saved at epoch 39 with loss 0.7562\n",
            "Epoch 40 || Loss: 0.7537\n",
            "🏆 Best model saved at epoch 40 with loss 0.7537\n",
            "Epoch 41 || Loss: 0.7520\n",
            "🏆 Best model saved at epoch 41 with loss 0.7520\n",
            "Epoch 42 || Loss: 0.7514\n",
            "🏆 Best model saved at epoch 42 with loss 0.7514\n",
            "Epoch 43 || Loss: 0.7500\n",
            "🏆 Best model saved at epoch 43 with loss 0.7500\n",
            "Epoch 44 || Loss: 0.7488\n",
            "🏆 Best model saved at epoch 44 with loss 0.7488\n",
            "Epoch 45 || Loss: 0.7461\n",
            "🏆 Best model saved at epoch 45 with loss 0.7461\n",
            "Epoch 46 || Loss: 0.7465\n",
            "Epoch 47 || Loss: 0.7453\n",
            "🏆 Best model saved at epoch 47 with loss 0.7453\n",
            "Epoch 48 || Loss: 0.7429\n",
            "🏆 Best model saved at epoch 48 with loss 0.7429\n",
            "Epoch 49 || Loss: 0.7421\n",
            "🏆 Best model saved at epoch 49 with loss 0.7421\n",
            "✅ Final Loss: 0.7421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiVzOJdgV5xP"
      },
      "outputs": [],
      "source": [
        "def translate(model , src , src_tokenizer , tgt_tokenizer , seq_length , device):\n",
        "  model.eval()\n",
        "  src_ids = src_tokenizer.encode(src).ids\n",
        "\n",
        "  src_tokens = [src_tokenizer.token_to_id(\"[SOS]\")] + src_ids + [src_tokenizer.token_to_id(\"[EOS]\")]\n",
        "  src_tokens += [src_tokenizer.token_to_id(\"[PAD]\")] * (seq_length - len(src_tokens))\n",
        "  encodr_input = torch.tensor(src_tokens , dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "  encoder_mask = (encodr_input != src_tokenizer.token_to_id(\"[PAD]\")).unsqueeze(0).unsqueeze(0).int().to(device)\n",
        "\n",
        "  tgt_tokens = [tgt_tokenizer.token_to_id(\"[SOS]\")]\n",
        "  for _ in range(seq_length):\n",
        "    decoder_input = torch.tensor(tgt_tokens , dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    tgt_mask = (decoder_input != tgt_tokenizer.token_to_id(\"[PAD]\")).unsqueeze(0).int().to(device)\n",
        "    tgt_casual_mask = casual_mask(decoder_input.size(1)).int().to(device)\n",
        "\n",
        "    decoder_mask = tgt_mask & tgt_casual_mask\n",
        "\n",
        "    # forward pass\n",
        "    with torch.no_grad():\n",
        "      encoder_output_translated = model.encode(encodr_input , encoder_mask)\n",
        "      output = model.decode(decoder_input , encoder_output_translated , encoder_mask , decoder_mask)\n",
        "\n",
        "\n",
        "    next_token_logits = output[0 , -1 , :]\n",
        "\n",
        "    # Apply softmax to the logits before sampling to get probabilities\n",
        "    probabilities = torch.softmax(next_token_logits, dim=-1)\n",
        "    next_token_id = torch.multinomial(probabilities , num_samples=1).item()\n",
        "\n",
        "    tgt_tokens.append(next_token_id)\n",
        "\n",
        "    # Use token_to_id instead of _get_token_id\n",
        "    if next_token_id == tgt_tokenizer.token_to_id(\"[EOS]\"):\n",
        "      break\n",
        "\n",
        "  decoder_token = tgt_tokenizer.decode(tgt_tokens[1:])\n",
        "  return decoder_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmKFZdbtyGs1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18100c81-d93c-4e94-9ca2-95daf90f4f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "मैंने रही थे मेरी तो मैंने तुम्हें है सकता रही थे मेरी थे गया तो गया गया थे गया मैंने मैंने थे रही मेरी तो गया ! के गया रही मैंने कर गया रही हम मैंने मैंने तुम पर टॉम । कर था रही ने रही थे ! ने यह से ? और रहा पर बहुत ने मैंने को किया गया ने थे को था थे रही थे पर और टॉम मैंने और बहुत थे रही थे पर रहा गया पर मैंने तो रहा\n"
          ]
        }
      ],
      "source": [
        "print(translate(model , \"I am hungry\" , eng_tokenizer , hindi_tokenizer , SEQ_LENGTH , device))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "cpu_losses = [loss.item() if isinstance(loss, torch.Tensor) else loss for loss in losses]\n",
        "\n",
        "plt.plot(cpu_losses)\n",
        "plt.xlabel(\"Batch Iteration\") # Optional: Add labels for clarity\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss over Iterations\")\n",
        "plt.show() # Add plt.show() to display the plot\n",
        "\n",
        "# for i in cpu_losses:\n",
        "#   print(i)"
      ],
      "metadata": {
        "id": "6dAGvv7lxjqO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "63c83f67-36eb-4deb-f6c9-5a950d47df09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMkXVhsJ3G74PNbTqJSwVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d2d5b5b8df349509b71d2e14faf32a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41aec81c130f44e392311317a933e260",
              "IPY_MODEL_5f5b64997dad4dd18f0d060177fefa9c",
              "IPY_MODEL_7c2390d44c58438691bf1d4a262b93e9"
            ],
            "layout": "IPY_MODEL_afa99b0f44db4b94b190ba02abee68c2"
          }
        },
        "41aec81c130f44e392311317a933e260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_422fe1b0eb5f4645b90422eff6372791",
            "placeholder": "​",
            "style": "IPY_MODEL_6bf64bb71b154b30a69079ce9fe9f47c",
            "value": "Training: 100%"
          }
        },
        "5f5b64997dad4dd18f0d060177fefa9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97ba9fc5a75448d0bd7fb3434b51b300",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_586db8fbd0db48929b6afa921fa5e8f4",
            "value": 50
          }
        },
        "7c2390d44c58438691bf1d4a262b93e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb41b4cf0a044f798e23a63c3343cc4f",
            "placeholder": "​",
            "style": "IPY_MODEL_08deb796708044978f01e651480174ec",
            "value": " 50/50 [16:27&lt;00:00, 19.86s/it]"
          }
        },
        "afa99b0f44db4b94b190ba02abee68c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "422fe1b0eb5f4645b90422eff6372791": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bf64bb71b154b30a69079ce9fe9f47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97ba9fc5a75448d0bd7fb3434b51b300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "586db8fbd0db48929b6afa921fa5e8f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb41b4cf0a044f798e23a63c3343cc4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08deb796708044978f01e651480174ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}